{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuations, stopwords, numbers and so on from the given text and doc files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tikle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tikle\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import important modules.\n",
    "import glob \n",
    "impImported important modules.ort re\n",
    "import string\n",
    "from zipfile import ZipFile \n",
    "import docx\n",
    "from docx import Document\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "import os\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the file which is saved in the local computer but we can use the functions for any text or doc documents \n",
    "with ZipFile('week_10_txt_and_docx.zip','r') as z:\n",
    "     z.extractall('week_10_txt_and_docx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the file names from the given folder\n",
    "text_file =glob.glob(r'week_10_txt_and_docx\\*.txt',recursive = True)\n",
    "doc_file= glob.glob(r'week_10_txt_and_docx\\*.docx',recursive = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['week_10_txt_and_docx\\\\52256-0.txt',\n",
       " 'week_10_txt_and_docx\\\\53031-0.txt',\n",
       " 'week_10_txt_and_docx\\\\58108-0.txt',\n",
       " 'week_10_txt_and_docx\\\\blind_text.txt',\n",
       " 'week_10_txt_and_docx\\\\dr_yawn.txt',\n",
       " 'week_10_txt_and_docx\\\\how_rubber_goods_are_made.txt',\n",
       " 'week_10_txt_and_docx\\\\most_boring_ever.txt',\n",
       " 'week_10_txt_and_docx\\\\most_boring_part2.txt',\n",
       " 'week_10_txt_and_docx\\\\pg12814.txt',\n",
       " 'week_10_txt_and_docx\\\\pg14895.txt',\n",
       " 'week_10_txt_and_docx\\\\pg43994.txt',\n",
       " 'week_10_txt_and_docx\\\\random_text.txt',\n",
       " 'week_10_txt_and_docx\\\\smiley_the_bunny.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file # text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['week_10_txt_and_docx\\\\week_10_document1.docx',\n",
       " 'week_10_txt_and_docx\\\\week_10_document2.docx']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " doc_file # document files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to clean the data from urls, phonenumbers,digits,white spaces etc\n",
    "def clean_data(data):\n",
    "    data=data.lower()\n",
    "    data= re.sub(r'(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*','', data)\n",
    "    data= re.sub(r'((1-\\d{3}-\\d{3}-\\d{4})|(\\(\\d{3}\\) \\d{3}-\\d{4})|(\\d{3}-\\d{3}-\\d{4}))$', '', data)\n",
    "    data = re.sub(r'[^\\w\\s]', '', data)\n",
    "    data= re.sub(\"\\d+\",'',  data)\n",
    "    data= filtered_data(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to clean the data from stopwords for text files\n",
    "def filtered_data(data):\n",
    "    sent_tokenizeList=sent_tokenize(data)\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    filtered_list = []\n",
    "    for word in sent_tokenizeList:\n",
    "        wordTokens=word_tokenize(word)\n",
    "        for w in wordTokens:\n",
    "            if w not in stop_words: \n",
    "                filtered_list.append(w) \n",
    "    return filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the file name and the corresponding list of words together and save them to filesystem\n",
    "for file in text_file:\n",
    "    with open(file, mode='r',encoding=\"utf8\", errors='ignore') as f:\n",
    "        \n",
    "        datat= f.read()\n",
    "        datat= clean_data(datat)\n",
    "        datat= list(set(datat))\n",
    "        filename=file.split('\\\\')[1]\n",
    "        \n",
    "        mode='a' if os.path.exists('data_file.dat') else 'w' \n",
    "        f=open('data_file.dat',mode,errors = 'ignore')\n",
    "        print('\"'+filename+'\": ',*datat,sep=',',end='\\n',file=f)\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the file name and the corresponding list of words for document files and save them to filesystem by .dat extension\n",
    "docdata= []\n",
    "for file in doc_file:\n",
    "    doc = Document(file)\n",
    "    for para in doc.paragraphs:\n",
    "        data=str(para.text) \n",
    "        data= clean_data(data)\n",
    "        docdata= docdata+data\n",
    "    docdata= list(set(docdata))\n",
    "    filename=file.split('\\\\')[1]\n",
    "    f=open('data_file.dat','a',errors = 'ignore')\n",
    "    print('\"'+filename+'\":' ,*docdata,sep=',', end='\\n',file=f)\n",
    "    f.write('\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!type data_file.dat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
